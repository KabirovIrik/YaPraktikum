{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "### Инструкция по выполнению проекта\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier , Pool\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы не сбаланасированны.<br>\n",
    "Сами данные могут содержать спецсимволы, например **\\n** или **\\r**. <br>\n",
    "Также слова могут быть соединены не только пробелами, но и другими символами<br>\n",
    "Возможно стоит заменить все числовые строки на одно число, чтобы сделать акцент именно на словах<br>\n",
    "В общем, следует почистить строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#приведем все в нижний регистр\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explanation\n",
      "why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"explanation . why the edits made under my username hardcore metallica fan were reverted ? they were not vandalism , just closure on some gas after i voted at new york doll fac . and please do not remove the template from the talk page since i 'm retired now . 9999 . 9999 . 9999 . 9999\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_replace(data):\n",
    "    # функция для рег.выражений\n",
    "    return data.group(0)[0]\n",
    "\n",
    "def get_lemm(text):\n",
    "    \"\"\"\n",
    "    Подготив строки, токенизируем и проведем лемматизацию\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[_|#|\\r|\\t|\\/]', ' ', text) # Заменим различные символы на пробел\n",
    "    text = re.sub(r'\\.', ' . ', text) # Добавим пробел вокруг точек\n",
    "    text = re.sub(r'\\,', ' , ', text) # Добавим пробел вокруг запятых\n",
    "    text = re.sub(r'\\n', ' . ', text) # Заменим переносы строк на точки\n",
    "    text = re.sub(r'n\\'t', ' not ', text) # например don't => do not \n",
    "    text = re.sub(r'\\b[0-9]\\w*|\\w*[0-9]', ' 9999 ', text) # все слова содержащие числа будут заменены на число 9999\n",
    "    text = re.sub(r'a{3,}|b{3,}|c{3,}|\\\n",
    "         d{3,}|e{3,}|f{3,}|\\\n",
    "         g{3,}|h{3,}|i{3,}|\\\n",
    "         j{3,}|k{3,}|l{3,}|\\\n",
    "         m{3,}|n{3,}|o{3,}|\\\n",
    "         p{3,}|q{3,}|r{3,}|\\\n",
    "         s{3,}|t{3,}|u{3,}|\\\n",
    "         v{3,}|w{3,}|x{3,}|\\\n",
    "         y{3,}|z{3,}', my_replace, text) # Если буква повторяется в слове 3 раз или более, то заменяем только на 1 повтор\n",
    "                                         # Например nooooo => no\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "\n",
    "# Проверим\n",
    "print(df.iloc[0]['text'])\n",
    "get_lemm(df.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 55s, sys: 728 ms, total: 4min 56s\n",
      "Wall time: 4min 58s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation . why the edits made under my user...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww ! he match this background colour i 'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man , i 'm really not trying to edit war ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>`` . more . i ca not make any real suggestion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you , sir , are my hero . any chance you remem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  explanation\\nwhy the edits made under my usern...      0   \n",
       "1  d'aww! he matches this background colour i'm s...      0   \n",
       "2  hey man, i'm really not trying to edit war. it...      0   \n",
       "3  \"\\nmore\\ni can't make any real suggestions on ...      0   \n",
       "4  you, sir, are my hero. any chance you remember...      0   \n",
       "\n",
       "                                          text_lemma  \n",
       "0  explanation . why the edits made under my user...  \n",
       "1  d'aww ! he match this background colour i 'm s...  \n",
       "2  hey man , i 'm really not trying to edit war ....  \n",
       "3  `` . more . i ca not make any real suggestion ...  \n",
       "4  you , sir , are my hero . any chance you remem...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df['text_lemma'] = df['text'].apply(get_lemm)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим данные к обучению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text_lemma'],\n",
    "    df['toxic'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "    \n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=42)\n",
    "    \n",
    "    return features_upsampled, target_upsampled\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Баланс классов в итоге ухудшил модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords, min_df=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer обучим на тренировочной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = X_train.values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = count_tf_idf.transform(corpus)\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим признаки для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = X_test.values.astype('U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7415310749533208"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# обучите и протестируйте модель\n",
    "model = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "model.fit(tf_idf, y_train)\n",
    "y_pred = model.predict(tf_idf_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2960854\ttotal: 228ms\tremaining: 1m 53s\n",
      "10:\tlearn: 0.1868755\ttotal: 1.63s\tremaining: 1m 12s\n",
      "20:\tlearn: 0.1652825\ttotal: 3.11s\tremaining: 1m 10s\n",
      "30:\tlearn: 0.1536957\ttotal: 4.58s\tremaining: 1m 9s\n",
      "40:\tlearn: 0.1442116\ttotal: 6.03s\tremaining: 1m 7s\n",
      "50:\tlearn: 0.1387072\ttotal: 7.44s\tremaining: 1m 5s\n",
      "60:\tlearn: 0.1348312\ttotal: 8.83s\tremaining: 1m 3s\n",
      "70:\tlearn: 0.1315098\ttotal: 10.2s\tremaining: 1m 1s\n",
      "80:\tlearn: 0.1282880\ttotal: 11.7s\tremaining: 1m\n",
      "90:\tlearn: 0.1259588\ttotal: 13.1s\tremaining: 58.7s\n",
      "100:\tlearn: 0.1235569\ttotal: 14.5s\tremaining: 57.3s\n",
      "110:\tlearn: 0.1212450\ttotal: 15.9s\tremaining: 55.8s\n",
      "120:\tlearn: 0.1195540\ttotal: 17.3s\tremaining: 54.2s\n",
      "130:\tlearn: 0.1176325\ttotal: 18.7s\tremaining: 52.7s\n",
      "140:\tlearn: 0.1163747\ttotal: 20.1s\tremaining: 51.3s\n",
      "150:\tlearn: 0.1148145\ttotal: 21.5s\tremaining: 49.8s\n",
      "160:\tlearn: 0.1130164\ttotal: 22.9s\tremaining: 48.3s\n",
      "170:\tlearn: 0.1120260\ttotal: 24.3s\tremaining: 46.8s\n",
      "180:\tlearn: 0.1101967\ttotal: 25.8s\tremaining: 45.4s\n",
      "190:\tlearn: 0.1093257\ttotal: 27.2s\tremaining: 43.9s\n",
      "200:\tlearn: 0.1083804\ttotal: 28.6s\tremaining: 42.5s\n",
      "210:\tlearn: 0.1076543\ttotal: 30s\tremaining: 41.1s\n",
      "220:\tlearn: 0.1068163\ttotal: 31.5s\tremaining: 39.8s\n",
      "230:\tlearn: 0.1062781\ttotal: 32.9s\tremaining: 38.4s\n",
      "240:\tlearn: 0.1055451\ttotal: 34.4s\tremaining: 37s\n",
      "250:\tlearn: 0.1046168\ttotal: 35.8s\tremaining: 35.5s\n",
      "260:\tlearn: 0.1038273\ttotal: 37.2s\tremaining: 34.1s\n",
      "270:\tlearn: 0.1030178\ttotal: 38.7s\tremaining: 32.7s\n",
      "280:\tlearn: 0.1024058\ttotal: 40s\tremaining: 31.2s\n",
      "290:\tlearn: 0.1015302\ttotal: 41.5s\tremaining: 29.8s\n",
      "300:\tlearn: 0.1003212\ttotal: 42.9s\tremaining: 28.3s\n",
      "310:\tlearn: 0.0996497\ttotal: 44.3s\tremaining: 26.9s\n",
      "320:\tlearn: 0.0990306\ttotal: 45.7s\tremaining: 25.5s\n",
      "330:\tlearn: 0.0983939\ttotal: 47.1s\tremaining: 24.1s\n",
      "340:\tlearn: 0.0977719\ttotal: 48.5s\tremaining: 22.6s\n",
      "350:\tlearn: 0.0969544\ttotal: 49.9s\tremaining: 21.2s\n",
      "360:\tlearn: 0.0961247\ttotal: 51.3s\tremaining: 19.8s\n",
      "370:\tlearn: 0.0956736\ttotal: 52.7s\tremaining: 18.3s\n",
      "380:\tlearn: 0.0952352\ttotal: 54.1s\tremaining: 16.9s\n",
      "390:\tlearn: 0.0948657\ttotal: 55.6s\tremaining: 15.5s\n",
      "400:\tlearn: 0.0945461\ttotal: 57s\tremaining: 14.1s\n",
      "410:\tlearn: 0.0942542\ttotal: 58.5s\tremaining: 12.7s\n",
      "420:\tlearn: 0.0938526\ttotal: 60s\tremaining: 11.3s\n",
      "430:\tlearn: 0.0933439\ttotal: 1m 1s\tremaining: 9.83s\n",
      "440:\tlearn: 0.0927284\ttotal: 1m 2s\tremaining: 8.4s\n",
      "450:\tlearn: 0.0924002\ttotal: 1m 4s\tremaining: 6.98s\n",
      "460:\tlearn: 0.0920360\ttotal: 1m 5s\tremaining: 5.55s\n",
      "470:\tlearn: 0.0917453\ttotal: 1m 7s\tremaining: 4.13s\n",
      "480:\tlearn: 0.0912125\ttotal: 1m 8s\tremaining: 2.7s\n",
      "490:\tlearn: 0.0908291\ttotal: 1m 9s\tremaining: 1.28s\n",
      "499:\tlearn: 0.0905241\ttotal: 1m 11s\tremaining: 0us\n",
      "Wall time: 1min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x16e24d47588>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = CatBoostClassifier(\n",
    "    custom_metric='F1', \n",
    "    iterations=500, \n",
    "    depth=4, \n",
    "    l2_leaf_reg=8,\n",
    "    learning_rate=0.67, \n",
    "    random_seed=42,\n",
    "    bagging_temperature=.5\n",
    ")\n",
    "\n",
    "model.fit(tf_idf, y_train, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7641114982578397"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model.predict(tf_idf_test)\n",
    "f1_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.7, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "              validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_reg = xgb.XGBClassifier(\n",
    "    objective ='binary:logistic', \n",
    "    learning_rate = 0.7,\n",
    ")\n",
    "xg_reg.fit(tf_idf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7564856451054998"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = xg_reg.predict(tf_idf_test)\n",
    "f1_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка на адекватность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coin_model(data):\n",
    "    # Случайно выбирает класс\n",
    "    return np.random.randint(2, size=data.shape[0])\n",
    "\n",
    "def one_model(data):\n",
    "    # Выбирает всегда один класс\n",
    "    return np.ones(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Случайная модель 0.1647095663065602\n",
      "Константная модель 0.1845331209647601\n"
     ]
    }
   ],
   "source": [
    "predict_1 = coin_model(X_test)\n",
    "predict_2 = one_model(X_test)\n",
    "print('Случайная модель', f1_score(y_test, predict_1))\n",
    "print('Константная модель', f1_score(y_test, predict_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшая модель - CatBoostRegressor. <br>\n",
    "Модель даекватная, т.к. случайная и константная модели намного хуже."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
